{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi4Gpdb5jY2-",
    "outputId": "eeeb714d-8c15-4454-9671-a97cf7e81fd2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Yz1WR1gtKzVc"
   },
   "outputs": [],
   "source": [
    "articles_to_analyze = [\n",
    "    '1755_Lisbon_earthquake.txt',\n",
    "    '1896_Summer_Olympics.txt',\n",
    "    '1997_Pacific_hurricane_season.txt',\n",
    "    'Actinium.txt',\n",
    "    'Barracuda.txt',\n",
    "    'Basketball.txt',\n",
    "    'Bath_School_disaster.txt',\n",
    "    'Chicago.txt',\n",
    "    'Chocolate.txt',\n",
    "    'Diamond.txt',\n",
    "    'Dice.txt',\n",
    "    'Drinking_water.txt',\n",
    "    'Duchenne_muscular_dystrophy.txt',\n",
    "    'Geography_of_Ireland.txt',\n",
    "    'George_S_Richardson_engineer.txt',\n",
    "    'Giraffe.txt',\n",
    "    'Gunpowder.txt',\n",
    "    'Ordinal_number.txt',\n",
    "    'Osama_bin_Laden.txt',\n",
    "    'Palm_oil.txt',\n",
    "    'Peace.txt',\n",
    "    'Pellagra.txt',\n",
    "    'Phishing.txt',\n",
    "    'Plant.txt',\n",
    "    'Plato.txt',\n",
    "    'Pneumonia.txt',\n",
    "    'Poison_gas_in_World_War_I.txt',\n",
    "    'Politics.txt',\n",
    "    'Pollution.txt',\n",
    "    'Pompeii.txt',\n",
    "    'Recycling.txt',\n",
    "    'Red_Kite.txt',\n",
    "    'Rice.txt',\n",
    "    'Rio_de_Janeiro.txt',\n",
    "    'Robert_K_Beck.txt',\n",
    "    'Romeo_and_Juliet.txt',\n",
    "    'Rugby_World_Cup.txt',\n",
    "    'Rwandan_Genocide.txt',\n",
    "    'Salt.txt',\n",
    "    'Sand.txt',\n",
    "    'Santa_Claus.txt',\n",
    "    'Scooby-Doo.txt',\n",
    "    'Seed.txt',\n",
    "    'Sequoia.txt'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LmhLP20W9qt6"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess the text\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Function to chunk text into manageable parts\n",
    "def chunk_text(text, max_length):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_length):\n",
    "        yield ' '.join(words[i:i+max_length])\n",
    "\n",
    "# Function to adjust score with the lexicon\n",
    "def adjust_score_with_lexicon(text, score, positive_words, negative_words, max_adjustment=0.5):\n",
    "    words = text.split()\n",
    "    positive_count = sum(word in positive_words for word in words)\n",
    "    negative_count = sum(word in negative_words for word in words)\n",
    "\n",
    "    if abs(score) >= 0.2:\n",
    "        difference = positive_count - negative_count\n",
    "        adjustment = (np.log(abs(difference) + 1) / np.log(max_adjustment + 1)) * np.sign(difference)\n",
    "        adjustment = np.clip(adjustment, -max_adjustment, max_adjustment)\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "    score += adjustment\n",
    "    score = np.clip(score, -1, 1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oN0NdQbEA5po",
    "outputId": "e1838d11-c7fa-49a3-8271-321c0d31b434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/plaintext_articles/1755_Lisbon_earthquake.txt', '../data/plaintext_articles/1896_Summer_Olympics.txt', '../data/plaintext_articles/1997_Pacific_hurricane_season.txt', '../data/plaintext_articles/Actinium.txt', '../data/plaintext_articles/Barracuda.txt', '../data/plaintext_articles/Basketball.txt', '../data/plaintext_articles/Bath_School_disaster.txt', '../data/plaintext_articles/Chicago.txt', '../data/plaintext_articles/Chocolate.txt', '../data/plaintext_articles/Diamond.txt', '../data/plaintext_articles/Dice.txt', '../data/plaintext_articles/Drinking_water.txt', '../data/plaintext_articles/Duchenne_muscular_dystrophy.txt', '../data/plaintext_articles/Geography_of_Ireland.txt', '../data/plaintext_articles/George_S_Richardson_engineer.txt', '../data/plaintext_articles/Giraffe.txt', '../data/plaintext_articles/Gunpowder.txt', '../data/plaintext_articles/Ordinal_number.txt', '../data/plaintext_articles/Osama_bin_Laden.txt', '../data/plaintext_articles/Palm_oil.txt', '../data/plaintext_articles/Peace.txt', '../data/plaintext_articles/Pellagra.txt', '../data/plaintext_articles/Phishing.txt', '../data/plaintext_articles/Plant.txt', '../data/plaintext_articles/Plato.txt', '../data/plaintext_articles/Pneumonia.txt', '../data/plaintext_articles/Poison_gas_in_World_War_I.txt', '../data/plaintext_articles/Politics.txt', '../data/plaintext_articles/Pollution.txt', '../data/plaintext_articles/Pompeii.txt', '../data/plaintext_articles/Recycling.txt', '../data/plaintext_articles/Red_Kite.txt', '../data/plaintext_articles/Rice.txt', '../data/plaintext_articles/Rio_de_Janeiro.txt', '../data/plaintext_articles/Robert_K_Beck.txt', '../data/plaintext_articles/Romeo_and_Juliet.txt', '../data/plaintext_articles/Rugby_World_Cup.txt', '../data/plaintext_articles/Rwandan_Genocide.txt', '../data/plaintext_articles/Salt.txt', '../data/plaintext_articles/Sand.txt', '../data/plaintext_articles/Santa_Claus.txt', '../data/plaintext_articles/Scooby-Doo.txt', '../data/plaintext_articles/Seed.txt', '../data/plaintext_articles/Sequoia.txt']\n",
      "here 1\n",
      "here 2\n",
      "here 3\n",
      "here 4\n",
      "here 5\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "zip_file_path = '/content/plaintext_articles.zip'\n",
    "extract_dir = '/content/extracted_articles'\n",
    "\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# After extracting, get the list of all files in the directory\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(\"Extracted files:\", extracted_files)\n",
    "extracted_files = os.listdir(extract_dir +'/' + extracted_files[0])\n",
    "print(\"Extracted files:\", extracted_files)\n",
    "# Ensure the files to analyze are in the extracted files list\n",
    "text_files = [os.path.join(extract_dir +'/plaintext_articles' , file) for file in articles_to_analyze if file in extracted_files]\n",
    "print(text_files)\n",
    "\n",
    "\n",
    "# After extracting, get the list of all files in the directory\n",
    "# Ensure the files to analyze are in the extracted files list\n",
    "text_files = [os.path.join(content_dir , file) for file in articles_to_analyze]\n",
    "\n",
    "for file_path in text_files:\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Define the task and model\n",
    "    task = 'sentiment'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    # Load tokenizer and model from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    # Set the max_length for tokenization\n",
    "    max_length = 512\n",
    "\n",
    "    # Download and load the label mapping\n",
    "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "    # Load positive and negative words lists (assuming you have them as text files)\n",
    "    positive_words = set(open('/content/Positive words.txt').read().splitlines())\n",
    "    negative_words = set(open('/content/Negative words.txt').read().splitlines())\n",
    "\n",
    "    chunks = chunk_text(preprocess(text), max_length)\n",
    "    final_scores = np.zeros(len(labels))\n",
    "    for chunk in chunks:\n",
    "        encoded_input = tokenizer(chunk, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "        output = model(**encoded_input)\n",
    "        scores = softmax(output[0][0].detach().numpy())\n",
    "        final_scores += scores\n",
    "\n",
    "\n",
    "    final_scores /= final_scores.sum()\n",
    "\n",
    "    sentiment_score = final_scores[2] - final_scores[0]  # Positive score minus negative score\n",
    "    sentiment_score = adjust_score_with_lexicon(preprocess(text), sentiment_score, positive_words, negative_words)\n",
    "\n",
    "    print(f\"File: {os.path.basename(file_path)} - Sentiment score: {sentiment_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqaDXE2aK0Ce"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adaproject",
   "language": "python",
   "name": "adaproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
